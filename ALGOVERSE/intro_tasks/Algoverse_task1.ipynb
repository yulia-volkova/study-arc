{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install python-dotenv --quiet\n",
    "!pip install -U datasets --quiet\n",
    "!pip install datasets>=2.15.0 --quiet"
   ],
   "metadata": {
    "id": "BJf4AmdFyz4w"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note:\n",
    "\n",
    "* When exploring the dataset, I noticed that 4 questions had incosistent choice number and such questions were **discarded** as they do not fit our prompt structure. I Kept 295 out of 299 questions with exactly 4 choices.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4w3IaHNPEZXW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hSgGMoriw3rW"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "class OpenAIConfig:\n",
    "    def __init__(self, model_id: str, temperature: float = 0.0, api_key: str = None):\n",
    "        self.api_key = api_key\n",
    "        self.client = openai.OpenAI(api_key=self.api_key)\n",
    "        self.model_id = model_id\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def ask(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "config = OpenAIConfig(\n",
    "    model_id=\"id\",\n",
    "    temperature=0,\n",
    "    api_key=\"key\"\n",
    ")"
   ],
   "metadata": {
    "id": "dv1tQrkJyXXs"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  #  Question: <question>\n",
    "  #   A. <choice A>\n",
    "  #   B. <choice B>\n",
    "  #   C. <choice C>\n",
    "  #   D. <choice D>\n",
    "  #   Answer:\n",
    "\n",
    "def format_prompt(example):\n",
    "    choices = example['choices']['text']\n",
    "    prompt = (\n",
    "        f\"Question: {example['question'].strip()}\\n\"\n",
    "        f\"A. {choices[0]}\\n\"\n",
    "        f\"B. {choices[1]}\\n\"\n",
    "        f\"C. {choices[2]}\\n\"\n",
    "        f\"D. {choices[3]}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return prompt"
   ],
   "metadata": {
    "id": "nhMiR5bwADn6"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\")"
   ],
   "metadata": {
    "id": "2OYoFYcA0tcA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "80fdc323-654b-4ab4-db71-01627caea094"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "val_split = ds[\"validation\"]\n",
    "print(val_split[0])\n",
    "print(val_split.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fStiAfz65U5d",
    "outputId": "95bd2225-1e52-4872-f5fc-7af0de80bd51"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'id': 'Mercury_SC_407695', 'question': 'Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?', 'choices': {'text': ['Put the objects in groups.', 'Change the height of the ramp.', 'Choose different objects to roll.', 'Record the details of the investigation.'], 'label': ['A', 'B', 'C', 'D']}, 'answerKey': 'D'}\n",
      "(299, 4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below I investigate the validation dataset a bit.\n"
   ],
   "metadata": {
    "id": "2Spa68Fa-KYG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check that all examples have answer key\n",
    "has_all_keys = all([example[\"answerKey\"] is not None for example in val_split])\n",
    "print(\"All have ground truth:\", has_all_keys)\n",
    "missing = [i for i, example in enumerate(val_split) if example[\"answerKey\"] is None]\n",
    "print(\"Missing indexes:\", missing)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJW8Ndxm7dCq",
    "outputId": "f0671d10-17f3-4e2b-b785-bab8441deb85"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All have ground truth: True\n",
      "Missing indexes: []\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check that all examples have 4 choices for the answer to fit our prompt structure\n",
    "for example in val_split:\n",
    "    num_choices = len(example['choices']['text'])\n",
    "    if num_choices != 4:\n",
    "        print(f\"Question ID: {example['id']}\")\n",
    "        print(f\"Question: {example['question'].strip()}\")\n",
    "        print(f\"# Choices: {num_choices}\")\n",
    "        print(f\"Choices: {example['choices']['text']}\")\n",
    "        print(f\"Answer Key: {example['answerKey']}\")\n",
    "        print(\"-\" * 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqwH0oXW9TW4",
    "outputId": "c0e516ae-d989-496f-df1d-0d139e158111"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Question ID: NYSEDREGENTS_2014_4_4\n",
      "Question: Which state of matter has no definite volume and no definite shape?\n",
      "# Choices: 3\n",
      "Choices: ['gas', 'liquid', 'solid']\n",
      "Answer Key: A\n",
      "--------------------------------------------------\n",
      "Question ID: NYSEDREGENTS_2014_4_19\n",
      "Question: As kittens grow into cats, their body weight usually\n",
      "# Choices: 3\n",
      "Choices: ['decreases', 'increases', 'remains the same']\n",
      "Answer Key: B\n",
      "--------------------------------------------------\n",
      "Question ID: TIMSS_2003_8_pg29\n",
      "Question: Which of the following organs is NOT situated in the abdomen?\n",
      "# Choices: 5\n",
      "Choices: ['liver', 'kidney', 'stomach', 'bladder', 'heart']\n",
      "Answer Key: E\n",
      "--------------------------------------------------\n",
      "Question ID: NYSEDREGENTS_2014_4_28\n",
      "Question: Large birds have been eating small animals in an area. If all of the large birds died from a disease, the number of small animals in the area would probably\n",
      "# Choices: 3\n",
      "Choices: ['decrease', 'increase', 'remain the same']\n",
      "Answer Key: B\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Note: I excluded 4 questions that did not contain exactly 4 answer choices to maintain a consistent prompt format for the fine-tuned model."
   ],
   "metadata": {
    "id": "hagJO5R3-k3h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "clean_val_split = [example for example in val_split if len(example['choices']['text']) == 4]\n",
    "print(f\"Kept {len(clean_val_split)} out of {len(val_split)} questions with exactly 4 choices.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFwk3UUJ_czb",
    "outputId": "cf73b6fe-89e6-47db-b22c-391edf6d36fd"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kept 295 out of 299 questions with exactly 4 choices.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = [format_prompt(q) for q in clean_val_split]\n",
    "ground_truth = [q['answerKey'] for q in clean_val_split]"
   ],
   "metadata": {
    "id": "ZB5NcHK25MbL"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "for prompt in tqdm(prompts, desc=\"Querying model\"):\n",
    "    answer = config.ask(prompt)\n",
    "    predictions.append(answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bo73UmHt5Mc8",
    "outputId": "faaafe4f-59c5-4013-ee28-6ad33673e73e"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Querying model: 100%|██████████| 295/295 [02:09<00:00,  2.27it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "correct = sum([pred == gt for pred, gt in zip(predictions, ground_truth)])\n",
    "accuracy = correct / len(ground_truth) * 100\n",
    "\n",
    "print(f\"Final accuracy on 100 samples: {accuracy:.1f}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0-vz2oqxkE8",
    "outputId": "5b37276e-79ee-44b6-8cb0-d79c6489f1b6"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final accuracy on 100 samples: 88.5%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Get incorrectly answered questions\n",
    "incorrect_questions = [\n",
    "    item[\"question\"]\n",
    "    for item, pred, gt in zip(clean_val_split, predictions, ground_truth)\n",
    "    if pred != gt\n",
    "]\n",
    "\n",
    "print(f\"Number of incorrectly answered questions: {len(incorrect_questions)}\")\n",
    "\n",
    "# Split into words, lowercase and count\n",
    "all_words = []\n",
    "\n",
    "for question in incorrect_questions:\n",
    "    words = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "    all_words.extend(words)\n",
    "\n",
    "counter = Counter(all_words)\n",
    "top5 = counter.most_common(5)\n",
    "\n",
    "formatted = \", \".join([f\"{word}: {count}\" for word, count in top5])\n",
    "print(f\"Top 5 most frequent words in incorrectly answered questions:\\n{formatted}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gc974_mHAlxR",
    "outputId": "a2ff1232-4594-46e2-f4c5-03df83dd39cb"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of incorrectly answered questions: 34\n",
      "Top 5 most frequent words in incorrectly answered questions:\n",
      "the: 68, of: 42, a: 31, is: 24, to: 21\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Top 5 most frequent words in incorrectly answered questions:\n",
    "\n",
    "**the: 67, of: 41, a: 30, is: 24, to: 21**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Final note: just word frequency might not be very informative, and another measure could be TF-IDF which downweights common words and highlights terms that are relatively special to the questions the model got wrong. For TF-IDF I got these top words and their respective scores:\n",
    "likely: 1.179, best: 0.980, stream: 0.881, water: 0.881, s: 0.794"
   ],
   "metadata": {
    "id": "xHezy0LdGOAP"
   }
  }
 ]
}
