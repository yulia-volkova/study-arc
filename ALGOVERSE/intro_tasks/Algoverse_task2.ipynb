{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-7962dsWbsd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.37.2 transformer_lens==2.11.0 --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_0juMbrkuPU"
      },
      "outputs": [],
      "source": [
        "from transformer_lens import (\n",
        "    ActivationCache,\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig,\n",
        ")\n",
        "import numpy as np\n",
        "import torch as t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81wJcyeXhYOD",
        "outputId": "f4ff445f-2f48-4506-f740-e7d6d7ac5ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n",
            "Moving model to device:  cuda\n"
          ]
        }
      ],
      "source": [
        "t.manual_seed(0)\n",
        "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icI2m1LKlJEw"
      },
      "outputs": [],
      "source": [
        "reference_text = \"The quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-eTSmLvlNmW",
        "outputId": "3155eb6a-f13b-45ad-ecf3-cdb0459bf930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['<|endoftext|>', 'The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy', ' dog']\n",
            "Target ' fox' token ID: 21831\n"
          ]
        }
      ],
      "source": [
        "tokens = model.to_tokens(reference_text).to(device)\n",
        "print(\"Tokens:\", [model.to_string(tok) for tok in tokens[0]])\n",
        "\n",
        "fox_token_id = model.to_single_token(\" fox\")\n",
        "print(f\"Target ' fox' token ID: {fox_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhsgysJ7nVxl"
      },
      "outputs": [],
      "source": [
        "# Forward pass: save all activations in cache\n",
        "logits, cache = model.run_with_cache(reference_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3laz8hna6l",
        "outputId": "50636d08-454e-469d-de61-289017256c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_U shape: torch.Size([768, 50257])\n"
          ]
        }
      ],
      "source": [
        "W_U = model.W_U  # [d_model, d_vocab]\n",
        "\n",
        "print(\"W_U shape:\", W_U.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmdmvrxbnpDw",
        "outputId": "fca2a0f7-2ea5-43cd-b81f-0edcfe61a7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer  0: P(' fox') = 4.227344030383051e-10\n",
            "Layer  1: P(' fox') = 3.8766975740678333e-11\n",
            "Layer  2: P(' fox') = 2.829986462185574e-12\n",
            "Layer  3: P(' fox') = 2.7017364040426983e-13\n",
            "Layer  4: P(' fox') = 7.391197851038e-16\n",
            "Layer  5: P(' fox') = 6.825884075029927e-20\n",
            "Layer  6: P(' fox') = 6.2337102183971546e-24\n",
            "Layer  7: P(' fox') = 3.872848673308404e-28\n",
            "Layer  8: P(' fox') = 1.5845348816598415e-31\n",
            "Layer  9: P(' fox') = 1.1578290039250028e-35\n",
            "Layer 10: P(' fox') = 8.898455443232237e-40\n",
            "Layer 11: P(' fox') = 2.9665488489756377e-40\n"
          ]
        }
      ],
      "source": [
        "# check when fox token starts to appear in the models predictions\n",
        "n_layers = model.cfg.n_layers\n",
        "layer_probs = []\n",
        "\n",
        "for layer in range(n_layers):\n",
        "  resid_post = cache[f\"blocks.{layer}.hook_resid_post\"].to(device)\n",
        "  hidden_state = resid_post[0,1,:] # second token at 1st pos\n",
        "\n",
        "  logits = hidden_state@ W_U # [d_vocab], we decode at each layer\n",
        "  probs = t.softmax(logits, dim =-1)\n",
        "\n",
        "  fox_prob = probs[fox_token_id].item()  # the chance ' fox' is next, given what the model knows so far\n",
        "\n",
        "  layer_probs.append(fox_prob)\n",
        "\n",
        "  print(f\"Layer {layer:2d}: P(' fox') = {fox_prob}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaowgCvsrmAt",
        "outputId": "93e8cada-af14-400b-9f43-171db179a9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max P(' fox'): 4.227344030383051e-10, at layer 0\n"
          ]
        }
      ],
      "source": [
        "# actually max prob is at layer 0 since model learns that ' fox' is unlikely to go after second token as we progress through the layers\n",
        "max_prob = max(layer_probs)\n",
        "max_layer = layer_probs.index(max_prob)\n",
        "print(f\"Max P(' fox'): {max_prob}, at layer {max_layer}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
